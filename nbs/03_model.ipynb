{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ConvNet for the 2020s\n",
    "\n",
    "Based on article [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) and official [code repository](https://github.dev/facebookresearch/ConvNeXt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "from typing import List, Tuple, Sequence, Dict, Any, Optional, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Truncated normal initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init(scale=.02):\n",
    "    'Scaled truncated normal initailizer'\n",
    "    return nn.initializers.variance_scaling(scale, 'fan_in', 'truncated_normal')\n",
    "\n",
    "scale = .02\n",
    "default_kernel_init = init(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"init\" class=\"doc_header\"><code>init</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>init</code>(**`scale`**=*`0.02`*)\n",
       "\n",
       "Scaled truncated normal initailizer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "conv = partial(nn.Conv, kernel_init=default_kernel_init)\n",
    "pw_conv = partial(nn.Dense, kernel_init=default_kernel_init)\n",
    "sw_drop = partial(nn.Dropout, broadcast_dims=(1,2,3))\n",
    "norm = nn.LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNeXt block\n",
    "> With all of these preparations, the benefit of adopting larger kernel-sized convolutions is significant. We will use 7x7 depthwise conv in each block.\n",
    ">\n",
    "> We will now use a single GELU activation in each block.\n",
    ">\n",
    "> From now on, we will use one LayerNorm as our choice of normalization in each residual block.\n",
    "\n",
    "![ConvNeXt Block](../images/ConvNeXtBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvBlock(nn.Module):\n",
    "    'Residual block with depthwise convolution, samplewise norm and dropout and gelu activation.'\n",
    "    dim: int = 3  # number of output features\n",
    "    drop: float = 0.0  # dropout rate\n",
    "    scale: float = 1e-6  # initial scale of direct path\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        res = x\n",
    "        x = conv(self.dim, (7, 7), padding='SAME', feature_group_count=self.dim, name='dw_conv')(x)\n",
    "        x = norm(name='lr_norm')(x)\n",
    "        x = pw_conv(self.dim*4, name='fc_1')(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = pw_conv(self.dim, name='fc_2')(x)\n",
    "        if self.scale > 0.:\n",
    "            gamma = self.param('gamma', init_fn=lambda _:self.scale * jnp.ones(self.dim))\n",
    "            x = x * gamma\n",
    "        if self.drop > 0.:\n",
    "            x = sw_drop(self.drop, name='sw_drop')(x, deterministic=not train)\n",
    "        return res + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"ConvBlock\" class=\"doc_header\"><code>class</code> <code>ConvBlock</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>ConvBlock</code>(**`dim`**:`int`=*`3`*, **`drop`**:`float`=*`0.0`*, **`scale`**:`float`=*`1e-06`*, **`parent`**:`Union`\\[`typing.Type[flax.linen.module.Module]`, `typing.Type[flax.core.scope.Scope]`, `typing.Type[flax.linen.module._Sentinel]`, `NoneType`\\]=*`<flax.linen.module._Sentinel object at 0x7ff17a97ca30>`*, **`name`**:`str`=*`None`*) :: `Module`\n",
       "\n",
       "Residual block with depthwise convolution, samplewise norm and dropout and gelu activation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ConvBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ConvBlock(64)\n",
    "dummy_x = jnp.ones((12, 224, 224, 64)) # (batch, height, width, channels)\n",
    "out, params = m.init_with_output(random.PRNGKey(0), dummy_x)\n",
    "out.shape, jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvStage(nn.Module):\n",
    "    'Convolutional stage'\n",
    "    dim: int # number of output features\n",
    "    depth: int # number of `ConvBlock`s\n",
    "    drops: Union[Sequence[float], None] = None # dropout rates \n",
    "    scale: float = 1e-6 # initial scale of direct path\n",
    "    downsample: bool = True # whether to downsample input\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool =True):\n",
    "        if self.downsample:\n",
    "            x = norm(name='lr_norm')(x)\n",
    "            x = conv(self.dim, kernel_size=(2, 2), strides=2, name='dwsample')(x)\n",
    "        drops = self.drops or [0.] * self.depth\n",
    "        for i, drop in enumerate(drops):\n",
    "            x = ConvBlock(dim=self.dim, drop=drop, scale=self.scale, name=f'block_{i}')(x, train=train)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 32, 32, 192),\n",
       " FrozenDict({\n",
       "     params: {\n",
       "         block_0: {\n",
       "             dw_conv: {\n",
       "                 bias: (192,),\n",
       "                 kernel: (7, 7, 1, 192),\n",
       "             },\n",
       "             fc_1: {\n",
       "                 bias: (768,),\n",
       "                 kernel: (192, 768),\n",
       "             },\n",
       "             fc_2: {\n",
       "                 bias: (192,),\n",
       "                 kernel: (768, 192),\n",
       "             },\n",
       "             gamma: (192,),\n",
       "             lr_norm: {\n",
       "                 bias: (192,),\n",
       "                 scale: (192,),\n",
       "             },\n",
       "         },\n",
       "         dwsample: {\n",
       "             bias: (192,),\n",
       "             kernel: (2, 2, 96, 192),\n",
       "         },\n",
       "         lr_norm: {\n",
       "             bias: (96,),\n",
       "             scale: (96,),\n",
       "         },\n",
       "     },\n",
       " }))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ConvStage(dim=192, depth=1)\n",
    "dummy_x = jnp.ones((7, 64, 64, 96)) # (batch, height, width, channels)\n",
    "out, params = m.init_with_output({'params':random.PRNGKey(0),'dropout':random.PRNGKey(1)}, dummy_x)\n",
    "out.shape, jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNeXt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Stem(nn.Module):\n",
    "    'Stem module'\n",
    "    dim: int \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = conv(self.dim, kernel_size=(4, 4), strides=4, name='conv_1')(x)\n",
    "        x = norm(name='lr_norm_1')(x)\n",
    "        return x\n",
    "\n",
    "class Head(nn.Module):\n",
    "    'Head module'\n",
    "    classes: int \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = norm(name='lr_norm')(x.mean(axis=(2, 3)))\n",
    "        x = nn.Dense(self.classes, kernel_init=default_kernel_init, name='out')(x)\n",
    "        return x\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    classes: int = 10 # number of classes\n",
    "    depths: Sequence[int] = (3, 3, 9, 3) # number of `ConvBlock`s per stage\n",
    "    dims: Sequence[int] = (96, 192, 384, 768) # number of output features per stage\n",
    "    drop: float = 0. # dropout rate\n",
    "    scale: float = 1e-6 # initial scale of direct path\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        x = Stem(dim=self.dims[0], name='stem')(x)\n",
    "        dep_sums = np.cumsum(self.depths)\n",
    "        drops = jnp.linspace(0., self.drop, dep_sums[-1]).split(dep_sums[:-1])\n",
    "        for i, (dim, depth, drop) in enumerate(zip(self.dims, self.depths, drops)):\n",
    "            x = ConvStage(dim=dim, depth=depth, drops=drop.tolist(), scale=self.scale, downsample=(i>0), name=f'stage_{i}')(x, train=train)\n",
    "        x = Head(classes=self.classes, name='head')(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ConvNeXt()\n",
    "dummy_x = jnp.ones((7, 224, 224, 3)) # (batch, height, width, channels)\n",
    "out, params = m.init_with_output({'params':random.PRNGKey(0),'dropout':random.PRNGKey(1)}, dummy_x)\n",
    "out.shape, jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_dataloaders.ipynb.\n",
      "Converted 02_experiment.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script('*.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
