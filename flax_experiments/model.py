# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_model.ipynb (unless otherwise specified).

__all__ = ['init', 'scale', 'default_kernel_init', 'conv', 'pw_conv', 'sw_drop', 'norm', 'ConvBlock', 'Stem', 'Head',
           'ConvNeXt']

# Cell
from functools import partial
from typing import List, Tuple, Sequence, Dict, Any, Optional, Union, Callable

import numpy as np

import jax
import jax.numpy as jnp
import jax.random as random

import flax
import flax.linen as nn

import optax

# Cell
def init(scale=.02):
    'Scaled truncated normal initailizer'
    return nn.initializers.variance_scaling(scale, 'fan_in', 'truncated_normal')

scale = .02
default_kernel_init = init(scale)

# Cell
conv = partial(nn.Conv, kernel_init=default_kernel_init)
pw_conv = partial(nn.Dense, kernel_init=default_kernel_init)
sw_drop = partial(nn.Dropout, broadcast_dims=(1,2,3))
norm = nn.LayerNorm

# Cell
class ConvBlock(nn.Module):
    'Residual block with depthwise convolution, samplewise norm and dropout and gelu activation.'
    dim: int = 3  # number of output features
    drop: float = 0.0  # dropout rate
    scale: float = 1e-6  # initial scale of direct path

    @nn.compact
    def __call__(self, x, train: bool = True):
        res = x
        x = conv(self.dim, (7, 7), padding='SAME', feature_group_count=self.dim, name='dw_conv')(x)
        x = norm(name='lr_norm')(x)
        x = pw_conv(self.dim*4, name='fc_1')(x)
        x = nn.gelu(x)
        x = pw_conv(self.dim, name='fc_2')(x)
        if self.scale > 0.:
            gamma = self.param('gamma', init_fn=lambda _:self.scale * jnp.ones(self.dim))
            x = x * gamma
        if self.drop > 0.:
            x = sw_drop(self.drop, name='sw_drop')(x, deterministic=not train)
        return res + x

# Cell
class Stem(nn.Module):
    'Stem module'
    dim: int
    @nn.compact
    def __call__(self, x):
        x = conv(self.dim, kernel_size=(4, 4), strides=4, name='conv_1')(x)
        x = norm(name='lr_norm_1')(x)
        return x

class Head(nn.Module):
    'Head module'
    classes: int
    @nn.compact
    def __call__(self, x):
        x = norm(name='lr_norm')(x.mean(axis=(2, 3)))
        x = nn.Dense(self.classes, kernel_init=default_kernel_init, name='out')(x)
        return x

class ConvNeXt(nn.Module):
    classes: int = 10 # number of classes
    depths: Sequence[int] = (3, 3, 9, 3) # number of `ConvBlock`s per stage
    dims: Sequence[int] = (96, 192, 384, 768) # number of output features per stage
    drop: float = 0. # dropout rate
    scale: float = 1e-6 # initial scale of direct path

    @nn.compact
    def __call__(self, x, train=True):
        x = Stem(dim=self.dims[0], name='stem')(x)
        dep_sums = np.cumsum(self.depths)
        drops = jnp.linspace(0., self.drop, dep_sums[-1]).split(dep_sums[:-1])
        for i, (dim, depth, drop) in enumerate(zip(self.dims, self.depths, drops)):
            x = ConvStage(dim=dim, depth=depth, drops=drop.tolist(), scale=self.scale, downsample=(i>0), name=f'stage_{i}')(x, train=train)
        x = Head(classes=self.classes, name='head')(x)
        return x